# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wdK3gyR84MlErEuvO6HtuPlh5sLsKayf
"""

from google.colab import drive
drive.mount('/content/drive')

#some extra libaries

from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D
from sklearn.model_selection import train_test_split

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import tensorflow as tf
from tensorflow import keras
from pathlib import Path
import PIL
import os

#load the dataset andb set the path

folder_path ="/content/drive/My Drive/project/dataset"

# import the neccesary libaries and set the path

from pathlib import Path
import pandas as pd
from tqdm import tqdm

folder_path = "/content/drive/My Drive/project/dataset"
glaucoma = Path(folder_path + '/glaucoma')
cataract = Path(folder_path + '/cataract')
normal = Path(folder_path + '/normal')
diabetic_retinopathy = Path(folder_path + '/diabetic_retinopathy')

disease_type = [glaucoma, cataract, normal, diabetic_retinopathy]
df = pd.DataFrame()

for types in disease_type:
    if types.exists():  # Check if the directory exists
        for imagepath in tqdm(list(types.iterdir()), desc=str(types)):
            df = pd.concat([df, pd.DataFrame({
                'image': [str(imagepath)],
                'disease_type': [disease_type.index(types)]
            })], ignore_index=True)

df

#inspect the data

df.head()

df.info()

df.describe()

df.shape

df.columns

df.count() # number of rows

df.disease_type.value_counts()

# check if null  values present are not.

df.isnull().sum()

#viusilize images in dataset

from tensorflow.keras.preprocessing.image import load_img, img_to_array

def plot_image(n, num_samples=3):
    diseases_lables = ['glaucoma','cataract', ' normal ',  ' diabetic_retinopathy']
    images = df[df['disease_type'] == n].sample(num_samples)['image']
    plt.figure(figsize=(15,6))

    for i, path in enumerate(images, 1):
      img= (plt.imread(path)- path.imread(path).min())/plt.imread(path).max()
      plt.subplot(3,3,i)
      plt.imshow(img)
      plt.axis('off')
      plt.title(diseases_lables[n])
      plt.show()

import seaborn as sns # visualization
import cv2
import tensorflow as tf
from tensorflow import keras
from pathlib import Path
import PIL
import os

import matplotlib.pyplot as plt

def plot_image(n, num_samples=3):
    disease_labels = ['glaucoma', 'cataract', 'normal', 'diabetic_retinopathy']
    images = df[df['disease_type'] == n].sample(num_samples)['image']
    plt.figure(figsize=(15, 6))

    for i, path in enumerate(images, 1):
        # Load the image using plt.imread
        img = plt.imread(path)

        # Normalize the image
        img = (img - img.min()) / img.max()

        # Plot the image
        plt.subplot(1, num_samples, i)
        plt.imshow(img)
        plt.axis('off')
        plt.title(disease_labels[n])

    plt.show()

plot_image(n=1, num_samples=3)

plot_image(3)

df.tail()

#show some samples:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import zipfile
import os
import cv2
import glob
import albumentations as A
from pathlib import Path
import tensorflow as tf
from sklearn.model_selection import train_test_split

from tensorflow.keras.applications import EfficientNetB3,InceptionResNetV2
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras import layers,regularizers
from tensorflow.keras import callbacks

from tensorflow.keras.preprocessing.image import ImageDataGenerator

def show_image_sample(df):
    file_column = 'image'  # Column with file paths
    label_column = 'disease_type'  # Column with labels

    random_data = df.sample(n=16)
    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10),
                             subplot_kw={'xticks': [], 'yticks': []})
    plt.suptitle('Samples of eye diseases!', y=1.05, fontsize=16)

    for i, ax in enumerate(axes.flat):
        ax.imshow(plt.imread(random_data.iloc[i][file_column]))
        ax.set_title(random_data.iloc[i][label_column])

    plt.tight_layout()
    plt.show()

show_image_sample(df)

## check for duplicates
df.duplicated().sum()

# data agumentation

import albumentations as A
import cv2
import matplotlib.pyplot as plt

def augment_image(image):
    aug = A.Compose([
        A.Flip(),
        A.Rotate(limit=75, always_apply=True),
    ])
    augmented_image = aug(image=image)['image']
    return augmented_image

def show_original_augment_image(df):
    file_column = 'image'

    random_data = df.sample(n=3)

    for i in range(len(random_data)):
        # Read the original image
        image = cv2.imread(random_data.iloc[i][file_column])


        augmented_image = augment_image(image)


        plt.subplot(1, 2, 1)
        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title("Original Image")

        plt.subplot(1, 2, 2)
        plt.imshow(cv2.cvtColor(augmented_image, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title("Augmented Image")

        plt.show()

show_original_augment_image(df)

df['disease_type']=df['disease_type'].map({0: 'glaucoma', 1:'cataract' , 2: 'normal', 3: 'diabetic_retinopathy' })

df.disease_type.value_counts()

# set target and feature values from dataset

#Feature selection

from sklearn.feature_selection import RFE, RFECV
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

#train-test data

from sklearn.model_selection import train_test_split

# Split your data into features (X) and labels (y)
x = df['image']  # 'image' contains image data
y = df['disease_type']  # 'disease_type' contains labels

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

x

y

#data augmentation using the ImageDataGenerator

from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)

datagen

#data generators for tarining

# Convert the 'disease_type' column to string
df['disease_type'] = df['disease_type'].astype(str)


train_data = datagen.flow_from_dataframe(dataframe=df,
                                         x_col='image',
                                         y_col='disease_type',
                                         target_size=(224, 224),
                                         class_mode='categorical',
                                         batch_size=32,
                                         shuffle=True,
                                         subset='training')

# Data generators for validation

df['disease_type'] = df['disease_type'].astype(str)

valid_data = datagen.flow_from_dataframe(dataframe=df,
                                          x_col ='image',
                                          y_col = 'disease_type',
                                          target_size=(224,224),
                                          class_mode = 'categorical',
                                          batch_size = 32,
                                          shuffle = True,
                                          subset = 'validation')

#BUILDING THE  MODEL

from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization, Dropout, RandomContrast, RandomRotation, RandomZoom, RandomFlip
from keras.models import Sequential

# labeling this

labels= [key for key in train_data.class_indices ]
num_classes= len(disease_type)

# 1st approach

from tensorflow.keras.applications.vgg19 import VGG19
image_size=224
vgg = VGG19(weights="imagenet",include_top = False,input_shape=(image_size,image_size,3))

# 2nd approach

from tensorflow.keras.applications import ResNet50

resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))

model = Sequential()
model.add(resnet)
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

model.summary()

# 3rd model tecnique

from tensorflow.keras.applications import InceptionV3

inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))

model = Sequential()
model.add(inception)
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# 4th approach

from tensorflow.keras.applications import MobileNetV2

mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))

model = Sequential()
model.add(mobilenet)
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))

# 5th approach

from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size, image_size, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))



for layer in vgg.layers:
    layer.trainable = False

# model summarization

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense


model= Sequential()
model.add(vgg)
model.add(Flatten())

model.add(Dense(256, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())

model.add(Dense(256,activation= 'relu'))
model.add(tf.keras.layers.BatchNormalization())

model.add(Dense(4,activation='softmax'))

model = tf.keras.Sequential([
    # Input layer, assuming input images of size 224x224x3
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    # Additional layers
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy'])

model.summary()

# data ploting using keras modeling techniques

tf.keras.utils.plot_model(model,
                          to_file="model.png",
                          show_shapes=True,
                          expand_nested=True)

# check the checkpoint technique in the model

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# ModelCheckpoint with updated file extension
checkpoint = ModelCheckpoint("vgg19.keras",  # Change .h5 to .keras
                             monitor="val_acc",
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=False,
                             save_freq='epoch')

# EarlyStopping callback
earlystop = EarlyStopping(monitor="val_acc",
                          patience=5,
                          verbose=1)

his = model.fit(
    train_data,       # Training data
    batch_size=32,    # Batch size for training
    epochs=20,        # Number of training epochs
    validation_data=valid_data,  # Validation data
    verbose=1,        # Verbosity level for training logs
    callbacks=[checkpoint, earlystop]  # Callback functions
)

his = model.fit(
    train_data,
    validation_data=valid_data,
    epochs = 10,
)

# check the model accuracy

loss, accuracy = model.evaluate(valid_data)
print("Loss:", loss)
print("Accuracy:", accuracy)

#model evalution

y_test = valid_data.classes
y_pred = model.predict(valid_data)
y_pred = np.argmax(y_pred,axis=1)

# model  evalution and validation in the from of visuilization of the model

from mlxtend.plotting import plot_confusion_matrix
cm = confusion_matrix(y_test,y_pred)
plot_confusion_matrix(conf_mat = cm,figsize=(8,7),class_names = ["glaucoma", "cataract", "normal", "diabetic_retinopathy"],
                      show_normed = True);
plt.savefig("Predected & True Label.png")

# check classification_report of the model

print(classification_report(y_test,y_pred,target_names = labels))

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame containing the 'image' and 'disease_type' columns
sns.barplot(x=df['disease_type'].value_counts().index, y=df['disease_type'].value_counts().values, palette="Set2")

plt.title("Class Distribution")
plt.xlabel("Disease Type")
plt.ylabel("Count")
plt.show()

# check training of the model using another method

from sklearn import svm
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix

# fine tuning with grid search
from sklearn.model_selection import GridSearchCV

g=np.arange(1e-4,1e-1,0.0001)
g=g.tolist()
parameters = {"kernel": ("sigmoid", "rbf", "poly", "linear"),
              "C": [1, 10, 0.1],
             "gamma": g}
method = svm.SVC()
grid_search = GridSearchCV(method, parameters)
grid_search.fit(x_train, y_train)

print(f"grid score:{grid_search.score}")

# ploting tecniques using this model

# Define early stopping callback
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(patience=5, restore_best_weights=True)

# Fit the model with callbacks
history = model.fit(train_data,
                    validation_data=valid_data,
                    epochs=5,
                    callbacks=[early_stopping])

# check how history can signifies this model

import matplotlib.pyplot as plt

def plot_training(history):
    # Check if the history object contains accuracy and loss data
    if 'accuracy' in history.history and 'val_accuracy' in history.history:
        # Plot accuracy
        plt.figure(figsize=(12, 6))

        plt.subplot(1, 2, 1)
        plt.plot(history.history['accuracy'], label='Train Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend(loc='lower right')

    if 'loss' in history.history and 'val_loss' in history.history:
        # Plot loss
        plt.subplot(1, 2, 2)
        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend(loc='upper right')

    plt.tight_layout()
    plt.show()

plot_training(history)

#

# Define epochs
epochs = range(1, len(history.history['loss']) + 1)

# Plot training & validation loss
plt.plot(epochs, history.history['loss'], label='Training Loss', marker='o')
plt.plot(epochs, history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()



# Plot training & validation accuracy
plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# plot with help of hist data

plt.plot(his.history['accuracy'])
plt.plot(his.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

# some extra plotting techniques to show by the uses of visualization process

# sactter plot

import matplotlib.pyplot as plt

epochs = range(1, len(his.history['accuracy']) + 1)

plt.scatter(epochs, his.history['accuracy'], label='Train Accuracy', color='blue', marker='o')
plt.scatter(epochs, his.history['val_accuracy'], label='Validation Accuracy', color='orange', marker='x')
plt.title('Model Accuracy (Scatter Plot)')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')
plt.show()

# bar plot

import numpy as np

# Assuming you want to plot accuracy at the last epoch
train_acc = his.history['accuracy'][-1]
val_acc = his.history['val_accuracy'][-1]

accuracy_data = {'Train Accuracy': train_acc, 'Validation Accuracy': val_acc}
names = list(accuracy_data.keys())
values = list(accuracy_data.values())

plt.bar(names, values, color=['blue', 'orange'])
plt.title('Final Epoch Accuracy')
plt.ylabel('Accuracy')
plt.show()

# heatmap

import seaborn as sns

data = {
    'Epoch': list(range(1, len(his.history['accuracy']) + 1)),
    'Train Accuracy': his.history['accuracy'],
    'Validation Accuracy': his.history['val_accuracy']
}

sns.heatmap(pd.DataFrame(data).set_index('Epoch').T, annot=True, cmap='YlGnBu')
plt.title('Model Accuracy Heatmap')
plt.ylabel('Accuracy Type')
plt.xlabel('Epoch')
plt.show()

# box plot

accuracy_data = [
    his.history['accuracy'],
    his.history['val_accuracy']
]

plt.boxplot(accuracy_data, labels=['Train Accuracy', 'Validation Accuracy'])
plt.title('Accuracy Distribution Across Epochs')
plt.ylabel('Accuracy')
plt.show()

# precision and recall measure

from sklearn.metrics import precision_score, recall_score, classification_report
import numpy as np
import matplotlib.pyplot as plt

print(x_test.shape)

x_test = x_test.reshape(844, 1)

x_test

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

y_pred = model.predict(x_test)

# Convert predictions to class indices
y_pred_classes = np.argmax(y_pred, axis=1)

# Assuming y_test is one-hot encoded
y_true_classes = np.argmax(y_test, axis=1)

# Calculate precision and recall
from sklearn.metrics import precision_score, recall_score, classification_report

precision = precision_score(y_true_classes, y_pred_classes, average='weighted')
recall = recall_score(y_true_classes, y_pred_classes, average='weighted')

print(f'Precision: {precision}')
print(f'Recall: {recall}')

# Print classification report
class_names = [...]
report = classification_report(y_true_classes, y_pred_classes, target_names=class_names)
print(report)





